{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Twitter Data Wrangling Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author:\n",
    "### Victor Mayowa, ADELEYE (MB;BS), Google Certified Data Analyst and Udacity Scholar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PREFACE\n",
    "> This project is my second project as a student of ALX-Udacity Data Analytics Nanodegree scholarship programme, which is a project intensive and career-focused training programme. It is instituted to provide and hone the skills of budding data analysts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "<ul>\n",
    "<li><a href=\"#intro\">Project Introduction</a></li>\n",
    "<li><a href=\"#gathering\">Data Gathering</a></li>\n",
    "<li><a href=\"#assess\">Data Assessment</a></li>\n",
    "<li><a href=\"#clean\">Data Cleaning</a></li>\n",
    "<li><a href=\"#storing\">Data Storage</a></li>\n",
    "<li><a href=\"#analysis\">Data Analysis</a></li>\n",
    "<li><a href=\"#visualizing\">Data Visualization</a></li>\n",
    "<li><a href=\"#report\">Data Analysis Report</a></li>\n",
    "<li><a href=\"#limitations\">Data Limitations</a></li>\n",
    "<li><a href=\"#conclusions\">Conclusions</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='intro'></a>\n",
    "\n",
    "## Project Introduction\n",
    "> \n",
    "> **PROJECT PURPOSE:** Rarely do data come in their cleanest form. Therefore, the purpose of the project is to test the data gathering via web scraping and crawling , using API keys and html files; also exercise the skills of data scanning, cleaning, analyzing, visualizing, storing and reporting. These are paramount skills to have as a data analytics.\n",
    ">\n",
    "> **PROJECT DETAILS:** The dataset that will be wrangled is the tweet archive of Twitter user [@dog_rates](https://twitter.com/dog_rates), also known as [WeRateDogs](https://en.wikipedia.org/wiki/WeRateDogs). WeRateDogs is a Twitter account that rates people's dogs with a humorous comment about the dog. These ratings almost always have a denominator of 10. The numerators, though? maybe greater than 10. 11/10, 12/10, 13/10, etc. Why? Because \"they're good dogs Brent.\" WeRateDogs has over 4 million followers and has received international media coverage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='gathering'></a>\n",
    "## Data Gathering\n",
    "> \n",
    "> **DATA SOURCE:** The data used in this project came from twitter archive and links provided by Udacity. Appropriate permissions from twitter and @WeRateDogs were ensured.\n",
    ">\n",
    "> There are three datasets required:\n",
    ">> **Twitter_archive_enhanced File:** The dataset was provided by Udacity, which was downloaded and imported into my workspace using python request and pandas packages respectively and stored as 'twt'. It contains columns such as (tweet_id, reply id, timestamp of tweet, source of tweet, retweet id, retweet timestamp, dog_connotations(doggo,pupper,puppo,floofer), nicknames and so on)\n",
    ">>\n",
    ">> **Image Prediction File:** Also provide by the platform, downloded and imported with request and pandas packages respectively and stored as 'img'. It contains data for the prediction of breed from the image sent with the urls, three models with their confidence interval and predictions.\n",
    ">>\n",
    ">> **Scraped Data:** These are data which I had to get API key from twitter and scrape the data. Twitter provided me access key which was used to extract data needed for my project, using the tweepy package. The data I extracted were the numbers of favourites, retweet annd the concomittant tweet_id. The data were then stored in a text file(tweet_json.txt), then extrated with pandas package.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='assess'></a>\n",
    "## Data Assessment\n",
    "\n",
    "1. I created a copy of each of the datasets provided\n",
    "2. This involved visually assessing each dataset after importation by viewing the \n",
    "3. Then I programmatically assess each dataset, using the .head() , .info(), .sample() e.t.c functions.\n",
    "4. I listed the quality and tidiness issues, messing up each datasets.\n",
    "---------------\n",
    "\n",
    "## Quality issues\n",
    "\n",
    "#### 'twt_copy' table from 'twitter_enhanced_archive.csv' file\n",
    "\n",
    "1. 'tweet_id', 'retweeted_status_user_id','retweeted_status_id' column in numerical datatype instead of strings.\n",
    "2. 'timestamp', 'retweeted_status_timestamp',  column in string datatype instead of datetime\n",
    "3.  Most(large percentage) of the observations in columns like in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp columns.\n",
    "4.  there are ratings_numerators showed to have ratings above 10. it was said to have been part of the rating system of organization. However, for the denominator which must be /10, I discovered inconsistencies as some observations have denominators less than and some more than 10.\n",
    "5. Some observations in 'name' column are in lower alphabetic case all through ( such as a, an, mad, the, very, one, not, actually, this, by, such, space, and so on ) which are inconsistent with other observationss with first letter capitalized. About 109 of the observations were implicated. There are 745 observations tagged 'None' i.e without name.\n",
    "6. The 'source' column is not tidy as the source of post is jumbled up with links and html junks.\n",
    "----\n",
    "#### 'img_copy' table from 'image-predictions.tsv' file\n",
    "\n",
    "7. 'tweet_id' in integer datatype instead of strings as an identification number \n",
    "8.  title of some columns(p1, p2, p3, p1_conf, p2_conf, p3_conf) in ' img_copy' table are not well defined for easy understanding.\n",
    "-------\n",
    "#### 'tweet_scraped_copy' table from scraped data\n",
    "9. the 'id' column is in numerical datatype instead of strings as an identification number.\n",
    "---------\n",
    "### Tidiness issues\n",
    "9. The 'source' column is not tidy as the source of post is jumbled up ### Tidiness issues\n",
    "10. the values of doggo, floofer, pupper and puppo int 'twt_copy' table should be combined into one column.\n",
    "11. the name of the identification column ('id') is different from the identification name given to the other tables provided ('tweet_id')\n",
    "12. combine all dataframe into one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='clean'></a>\n",
    "## Data Cleaning\n",
    ">\n",
    "> After visually and programmatically assesingg the datasets and the dirtiness of the data extracted, I decided to clean the data by defining, correcting via codes and testing my corrections for each of the issues listed in during assessment of the data.\n",
    "1. I corrected the datatypes of columns approprately using the astype function e.g ( numerical ids to string format and timestamps to datetime format.\n",
    "2. I used the drop function to remove columns with large percentage of null values.\n",
    "3. Since denominator must be 10. I drop some invalid data with value greater or less than 10\n",
    "4. The 'name' column in the Twitter_enhanced_archive dataset contains some invalid observations which were found to be in small letters. I used regex function to expose and remove them.\n",
    "5. Changed the name of the id in the scraped data to be in tandem with other datatsets using rename function\n",
    "6. The junks (such as the html codes and link) attacked to source of tweets in the twitter_enhance_archive file were exposed using regex function and pruned off.\n",
    "7. Merged  the dog nicknames into one column called dog_connotation while dropping the dogs with double nicknames.\n",
    "8. The names of the columns for the 3 model predictions and their confidence levels are changed for easy identification.\n",
    "9. Selected columns that are relevant for data analysis in the image prediction data while dropping others.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='storing'></a>\n",
    "## Data Storage\n",
    "> \n",
    ">This is where I combined my datasets into one and saved in a csv file called 'twitter_archive_master.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='analysis'></a>\n",
    "## Data Analysis\n",
    "\n",
    "This where I did and brought out some observable insights in my analysis. such as\n",
    "\n",
    "\n",
    "### Insights\n",
    "1. There is 78% positive correlation between number of tweets retweeted and liked. This suggest that there is a strong connection between a loiked and retweeted tweet such that a tweet with a lot of likes will most likely have a lot of retweets\n",
    "\n",
    "2. Pupper was found to be the most used among the nickname given to dogs. Although, large percentage of the tweets did not indicate nicknames.\n",
    "\n",
    "3. Most of the tweets came from Iphone users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='visualizing'></a>\n",
    "## Data Visualization\n",
    ">\n",
    "> Visualizations using barcharts, piechart, histogram was helpful in revealing the insights found during analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='report'></a>\n",
    "## Data Analysis Report\n",
    ">\n",
    "> There are 2 reports required for the completion of this project. They are the **Wrangle report** and the **'act report'**\n",
    "> Here is the the wrangle report. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='limitations'></a>\n",
    "## Data Limitations\n",
    "\n",
    "- Getting the scraped data from twitter was initially difficult as  twitter delayed provision of acess key to the archive. This was solved by Udacity who provided the dataset needed pending the time I got needed keys.\n",
    "- Had issues with the nicknames of dogs which had double entry. I was though able to get a knowledge of selecting data needed with np.select function.\n",
    "- Also had a little struggle with regex function usage.\n",
    "- Had to drop some columns which have large percentage of null values. Having the values would have help to gain more insight to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conclusions'></a>\n",
    "## Conclusions\n",
    ">\n",
    "> - The project was a skill honer. I had a succesful usage of web API and extracting data from websites.\n",
    "> - I was able to deal with new challenges in my data journey head on.\n",
    "> - I appreciate ALX-Udacity for the opportunity.\n",
    "> - My appreciation also goes to my session lead (Ahmed Ramzy) and career trainers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFERENCE\n",
    "> - https://www.youtube.com/watch?v=yCgJGsg0Xa4&list=PLXwISwBsZoZd5mOZY9QcnvgG4PB0gtONP&index=26\n",
    "> -  https://pbpython.com/pandas_dtypes.html#:~:text=%20In%20order%20to%20convert%20data%20types%20in,as%20to_numeric%20%28%29%20or%20to_datetime%20%28%29%20More%20\n",
    "> - https://www.geeksforgeeks.org/how-to-drop-rows-in-dataframe-by-conditions-on-column-values/\n",
    "> - https://towardsdatascience.com/creating-conditional-columns-on-pandas-with-numpy-select-and-where-methods-8ee6e2dbd5d5\n",
    "> - https://stackoverflow.com/questions/49535664/how-to-hyperlink-in-a-jupyter-notebook\n",
    "> - https://www.youtube.com/watch?v=6ATDaLTQm6A\n",
    "> - https://www.youtube.com/watch?v=jE-TIoRvL3k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
